{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Martin Loza\n",
    "\n",
    "Date: 23/12/28\n",
    "\n",
    "In this workflow I want to calculate the average matrix for the MicroHi-C data from the 4DN portal\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init libraries\n",
    "import cooler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Global variables\n",
    "data_dir = \"/mnt/d/Projects/HK_Interactions/Data/HiC/4DN_portal/raw/\"\n",
    "out_dir = \"/mnt/d/Projects/HK_Interactions/Data/HiC/4DN_portal/average/\"\n",
    "\n",
    "# Local functions\n",
    "\n",
    "# from Functions_Average import PreprocessPixels, GetAverageMatrix, GetResolutionsMcool\n",
    "\n",
    "def GetResolutionsMcool(mcool_path):\n",
    "    \"\"\"\n",
    "    Get the available resolutions from an mcool file.\n",
    "    \n",
    "    Parameters:\n",
    "    - mcool_path: str, path to the mcool file\n",
    "    \n",
    "    Returns:\n",
    "    - resolutions: list of int, available resolutions in the mcool file\n",
    "    \"\"\"\n",
    "    # Open the mcool file\n",
    "    with h5py.File(mcool_path, 'r') as tmp_mcool:\n",
    "        # Get the available resolutions\n",
    "        resolutions = tmp_mcool['resolutions'].keys()\n",
    "        # Change the resolutions to a vector\n",
    "        resolutions = [int(res) for res in resolutions]\n",
    "        # Order the resolutions\n",
    "        resolutions.sort()\n",
    "    \n",
    "    return resolutions\n",
    "\n",
    "def SameResolutionMcools(mcools_path):\n",
    "    \"\"\"\n",
    "    Compare the resolutions of multiple mcool files.\n",
    "    \n",
    "    Parameters:\n",
    "    - mcool_files: list of str, paths to the mcool files\n",
    "    \n",
    "    Returns:\n",
    "    - resolutions_equal: bool, True if all resolutions are equal, False otherwise\n",
    "    \"\"\"\n",
    "    resolutions_list = []\n",
    "    \n",
    "    for mcool_file in mcools_path:\n",
    "        resolutions = GetResolutionsMcool(mcool_file)\n",
    "        resolutions_list.append(resolutions)\n",
    "    \n",
    "    resolutions_equal = all(resolutions_list[i] == resolutions_list[i+1] for i in range(len(resolutions_list)-1))\n",
    "    \n",
    "    return resolutions_equal\n",
    "\n",
    "def ScaleByGroup(df, group_column):\n",
    "    \"\"\"\n",
    "    Scale the 'log10_balanced' values in a DataFrame by group.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pandas.DataFrame): The input DataFrame.\n",
    "    - group_column (str): The column name used for grouping.\n",
    "\n",
    "    Returns:\n",
    "    - scaled_matrix (pandas.DataFrame): The DataFrame with scaled 'log10_balanced' values.\n",
    "\n",
    "    \"\"\"\n",
    "    # Obtain the unique values of the group column\n",
    "    group_values = df[group_column].unique()\n",
    "\n",
    "    # Create a list to store the scaled matrices\n",
    "    scaled_matrices = []\n",
    "\n",
    "    # Loop through each group value\n",
    "    for value in group_values:\n",
    "        # Filter the DataFrame by group value\n",
    "        tmp_data = df[df[group_column] == value].copy()\n",
    "\n",
    "        # Scale the log10_balanced values\n",
    "        tmp_data['scaled'] = scaler.fit_transform(tmp_data['log10_balanced'].values.reshape(-1, 1))\n",
    "\n",
    "        # Append the scaled matrix to the list\n",
    "        scaled_matrices.append(tmp_data)\n",
    "\n",
    "    # Concatenate the scaled matrices\n",
    "    scaled_matrix = pd.concat(scaled_matrices)\n",
    "\n",
    "    return scaled_matrix\n",
    "\n",
    "def FilterIdNCells(df, n_cells=5, column='count', plot_histogram=True):\n",
    "    \"\"\"\n",
    "    Filters the DataFrame based on the number of elements per ID and returns the filtered DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame\n",
    "        The input DataFrame containing the data.\n",
    "    - n_cells: int, optional (default=5)\n",
    "        The number of cells required for an interaction ID to be included in the filtered DataFrame.\n",
    "    - column: str, optional (default='count')\n",
    "        The column in the DataFrame used for counting the number of elements per ID.\n",
    "    - plot_histogram: bool, optional (default=True)\n",
    "        Whether to plot a histogram of interaction IDs with less than n_cells elements.\n",
    "\n",
    "    Returns:\n",
    "    - df: DataFrame\n",
    "        The filtered DataFrame containing only the interaction IDs that appear in n_cells cell types.\n",
    "    \"\"\"\n",
    "    # count the number of elements per id\n",
    "    tmp = df.groupby('id')[column].count()\n",
    "    # ids of interaction appearing in n cell types\n",
    "    ids = tmp[tmp == n_cells].index\n",
    "    # filter the data\n",
    "    df = df[df['id'].isin(ids)]\n",
    "    \n",
    "    if plot_histogram:\n",
    "        # histogram of interaction in less than n cell types\n",
    "        plt.hist(tmp[tmp < n_cells], bins=10)\n",
    "        plt.title(f\"Distribution of Interaction IDs with Less than {n_cells} Elements\\nColumn: {column}\")\n",
    "        plt.xlabel(\"Number of Elements\")\n",
    "        plt.ylabel(\"Frequency\")\n",
    "        plt.show()\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Function to replace NaN values with a value outside the range of the data\n",
    "def ReplaceNaNs(df, column='scaled', replace_with=-10):\n",
    "    \"\"\"\n",
    "    Replaces NaN values in a specified column of a DataFrame with a specified value.\n",
    "    \n",
    "    Args:\n",
    "        df (pandas.DataFrame): The DataFrame containing the column with NaN values.\n",
    "        column (str): The name of the column to replace NaN values in. Default is 'scaled'.\n",
    "        replace_with (float): The value to replace NaN values with. Default is -10.\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: The DataFrame with NaN values replaced in the specified column.\n",
    "    \"\"\"\n",
    "        \n",
    "    #if the value is true, throw a warning and change the nan_value to something more appropriate\n",
    "    if df[column].min() < replace_with:\n",
    "        # Change the value based on the minimum value of the scaled data\n",
    "        replace_with = round(df[column].min() - 1)  \n",
    "        warnings.warn(f\"The replace_with is not outside the range of the scaled data. Changing the value to: {nan_value}\")\n",
    "        \n",
    "    # replace the NaN values with the nan_value\n",
    "    df[column] = df[column].fillna(replace_with)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def PreprocessPixels(metadata, chromosomes, resolution, test=False):\n",
    "    \"\"\"\n",
    "    Preprocesses the pixels of a matrix for a given cell type. Perform log10 transformation and scaling by chromosome.\n",
    "\n",
    "    Args:\n",
    "        metadata (DataFrame): The metadata containing information about the cell types and mcool file paths.\n",
    "        chromosomes (list): The list of chromosomes to include in the analysis.\n",
    "        resolution (int): The resolution of the mcool.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: The preprocessed and scaled matrix.\n",
    "\n",
    "    \"\"\"\n",
    "    # List to store scaled matrices\n",
    "    scaled_matrices = []\n",
    "\n",
    "    if test:\n",
    "        cell_index = [0]\n",
    "    else:\n",
    "        cell_index = list(range(len(metadata)))\n",
    "\n",
    "    # Loop through each cell type, e.g. each row in the metadata\n",
    "    for i in cell_index:\n",
    "        # Get the current cell type\n",
    "        cell_type = metadata['Biosource'][i]\n",
    "        # Get the current mcool file path\n",
    "        mcool_file = metadata['local_path'][i]\n",
    "           \n",
    "        # Load the mcool file\n",
    "        cool_data = cooler.Cooler(mcool_file + \"::resolutions/\" + str(resolution))\n",
    "            \n",
    "        # Load the balanced matrix\n",
    "        cool_matrix = cool_data.matrix(balance=True, as_pixels=True, join=True)[:]\n",
    "\n",
    "        # Remove pixels of chromosomes not included in the main analysis\n",
    "        cool_matrix = cool_matrix[cool_matrix['chrom1'].isin(chromosomes)].reset_index(drop=True)\n",
    "        cool_matrix = cool_matrix[cool_matrix['chrom2'].isin(chromosomes)].reset_index(drop=True) \n",
    "            \n",
    "        # remove inter-chromosomal interactions\n",
    "        cool_matrix = cool_matrix[cool_matrix['chrom1'] == cool_matrix['chrom2']].reset_index(drop=True) \n",
    "\n",
    "        # Add a column with the cell type\n",
    "        cool_matrix['cell_type'] = cell_type\n",
    "\n",
    "        # Calculate the log10 of balanced counts\n",
    "        cool_matrix['log10_balanced'] = cool_matrix.groupby('chrom1')['balanced'].transform(lambda x: np.log10(x))\n",
    "            \n",
    "        # Scale the log10 transformed values by chromosome using scale_by_group function\n",
    "        scaled_matrix = ScaleByGroup(cool_matrix, 'chrom1')\n",
    "            \n",
    "        # Append the scaled matrix to the list\n",
    "        scaled_matrices.append(scaled_matrix)\n",
    "\n",
    "    # Concatenate the scaled matrices\n",
    "    scaled_matrix = pd.concat(scaled_matrices)\n",
    "        \n",
    "    return scaled_matrix\n",
    "\n",
    "def GetAverageMatrix(scaled_data, metadata,  plot_histogram=True, replaceNaN_with=-10, filter_nCells=6):\n",
    "\n",
    "    # Let's add an interaction_id, then group by interaction (or loop by interaction) to average the scaled data.\n",
    "    # This can be a bit time consuming for high resolution bins.\n",
    "        \n",
    "    # add interaction id\n",
    "    scaled_data['id'] = scaled_data['chrom1'].astype(str) + '_' + scaled_data['start1'].astype(str) + '_' + scaled_data['chrom2'].astype(str) + '_' + scaled_data['start2'].astype(str)\n",
    "\n",
    "    #Filter interactions that doesn't appear in all cell types using the local function\n",
    "    scaled_data = FilterIdNCells(df = scaled_data, n_cells=filter_nCells, column='count', plot_histogram=plot_histogram)\n",
    "\n",
    "    # Replace NaN values with a value outside the range of the data\n",
    "    scaled_data = ReplaceNaNs(scaled_data, column='scaled', replace_with=replaceNaN_with)\n",
    "    # short test to make sure everything is working\n",
    "    test = scaled_data['scaled'].isna().sum()\n",
    "    if test > 0:\n",
    "        print(f\"There are {test} NaN values in the scaled column after replacing them with an arbitrary value.\")\n",
    "        # stop the loop if there are still NaN values\n",
    "        # break\n",
    "\n",
    "    # Group by interaction id and average the scaled values\n",
    "    mean_values = scaled_data.groupby('id')['scaled'].mean()\n",
    "    # Convert the mean_values series to a dataframe and reset the index\n",
    "    mean_values = mean_values.reset_index()\n",
    "    # Rename the columns\n",
    "    mean_values.columns = ['id', 'mean']\n",
    "\n",
    "    # Create a new dataframe to store the mean values\n",
    "    # we can filter the data for only one cell type and remove the unnecessary columns\n",
    "    data_mean = scaled_data[scaled_data['cell_type'] == metadata['Biosource'][0]].copy()\n",
    "    # select only the columns we need\n",
    "    data_mean = data_mean[['id', 'chrom1', 'start1', 'end1', 'chrom2', 'start2', 'end2']]\n",
    "\n",
    "    # Merge the mean_values with the data_mean dataframe\n",
    "    data_mean = data_mean.merge(mean_values, on='id', how='left')\n",
    "\n",
    "    # Select the columns we need\n",
    "    data_mean = data_mean[['chrom1', 'start1', 'end1', 'chrom2', 'start2', 'end2', 'mean']]\n",
    "\n",
    "    return data_mean\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the metadata. We need to skip the first row because it is a comment. \n",
    "# We use the first row as header\n",
    "metadata = pd.read_csv(data_dir + \"/metadata.tsv\", sep=\"\\t\", header=0, skiprows=1)[:]\n",
    "# Select the columns of interest\n",
    "metadata = metadata[['File Accession', 'Biosource']]\n",
    "# Remove the rows with NaN values\n",
    "metadata = metadata.dropna()\n",
    "# Add the file path to the metadata\n",
    "metadata['local_path'] = data_dir + metadata['File Accession'] + \".mcool\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1000,\n",
       " 2000,\n",
       " 5000,\n",
       " 10000,\n",
       " 25000,\n",
       " 50000,\n",
       " 100000,\n",
       " 250000,\n",
       " 500000,\n",
       " 1000000,\n",
       " 2500000,\n",
       " 5000000,\n",
       " 10000000]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SameResolutionMcools(metadata['local_path'])\n",
    "GetResolutionsMcool(metadata['local_path'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to loop over the resolutions, scale them and average over cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the chromosomes to analyse\n",
    "chromosomes = ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17','chr18','chr19','chr20','chr21','chr22','chrX']\n",
    "\n",
    "# Define the resolutions to use\n",
    "resolutions = GetResolutionsMcool(metadata['local_path'][0])\n",
    "\n",
    "#test resolution\n",
    "#arrange resolution from high to low\n",
    "resolutions.sort(reverse=True)\n",
    "# resolutions = [1000000]\n",
    "# resolutions = [ 100000, 50000, 25000, 10000,  5000, 2000, 1000]\n",
    "# resolutions = ['1000', '10000', '100000', '1000000', '2000', '25000', '250000', '2500000', '5000', '50000', '500000', '5000000']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10000000,\n",
       " 5000000,\n",
       " 2500000,\n",
       " 1000000,\n",
       " 500000,\n",
       " 250000,\n",
       " 100000,\n",
       " 50000,\n",
       " 25000,\n",
       " 10000,\n",
       " 5000,\n",
       " 2000,\n",
       " 1000]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find the average for different resolutions in a loop. This could be optimized to make it in multiple-cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each resolution, get the average matrix and save it to a bed file\n",
    "for resolution in resolutions:\n",
    "\n",
    "    # If average matrix already exists, skip the resolution\n",
    "    if os.path.exists(out_dir + f\"average_matrix_{resolution}.bed\"):\n",
    "        print(f\"Average matrix for resolution {resolution} already exists. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Preprocess the pixels\n",
    "    scaled_data = PreprocessPixels(metadata, chromosomes, resolution, test=False)\n",
    "    # Get the average matrix\n",
    "    average_matrix = GetAverageMatrix(scaled_data, metadata, replaceNaN_with=-10, plot_histogram=True, filter_nCells=len(metadata))\n",
    "    # Save the average matrix to a bed file\n",
    "    average_matrix.to_csv(out_dir + f\"average_matrix_{resolution}.bed\", sep='\\t', index=False, header=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HK_PPI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
