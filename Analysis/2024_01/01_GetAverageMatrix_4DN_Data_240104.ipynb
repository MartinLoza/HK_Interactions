{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Martin Loza\n",
    "\n",
    "Date: 23/12/31\n",
    "\n",
    "In this workflow I want to calculate the average matrix for the MicroHi-C data from the 4DN portal\n",
    "In previous test I failed to get the average on high resolution data as 10Kbs. I would like to optimize the workflow to be able to complete the task. \n",
    "One possible way is to perform the average on each chromosome. This should be OK, as we already discarted the intra-chromosomal interactions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init libraries\n",
    "import cooler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import os\n",
    "import gc\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Global variables\n",
    "data_dir = \"/mnt/d/Projects/HK_Interactions/Data/HiC/4DN_portal/raw/\"\n",
    "out_dir = \"/mnt/d/Projects/HK_Interactions/Data/HiC/4DN_portal/average/\"\n",
    "\n",
    "# Local functions\n",
    "\n",
    "# from Functions_Average import PreprocessPixels, GetAverageMatrix, GetResolutionsMcool\n",
    "\n",
    "def GetResolutionsMcool(mcool_path):\n",
    "    \"\"\"\n",
    "    Get the available resolutions from an mcool file.\n",
    "    \n",
    "    Parameters:\n",
    "    - mcool_path: str, path to the mcool file\n",
    "    \n",
    "    Returns:\n",
    "    - resolutions: list of int, available resolutions in the mcool file\n",
    "    \"\"\"\n",
    "    # Open the mcool file\n",
    "    with h5py.File(mcool_path, 'r') as tmp_mcool:\n",
    "        # Get the available resolutions\n",
    "        resolutions = tmp_mcool['resolutions'].keys()\n",
    "        # Change the resolutions to a vector\n",
    "        resolutions = [int(res) for res in resolutions]\n",
    "        # Order the resolutions\n",
    "        resolutions.sort()\n",
    "    \n",
    "    return resolutions\n",
    "\n",
    "def SameResolutionMcools(mcools_path):\n",
    "    \"\"\"\n",
    "    Compare the resolutions of multiple mcool files.\n",
    "    \n",
    "    Parameters:\n",
    "    - mcool_files: list of str, paths to the mcool files\n",
    "    \n",
    "    Returns:\n",
    "    - resolutions_equal: bool, True if all resolutions are equal, False otherwise\n",
    "    \"\"\"\n",
    "    resolutions_list = []\n",
    "    \n",
    "    for mcool_file in mcools_path:\n",
    "        resolutions = GetResolutionsMcool(mcool_file)\n",
    "        resolutions_list.append(resolutions)\n",
    "    \n",
    "    resolutions_equal = all(resolutions_list[i] == resolutions_list[i+1] for i in range(len(resolutions_list)-1))\n",
    "    \n",
    "    return resolutions_equal\n",
    "\n",
    "def ScaleByGroup(df, group_column):\n",
    "    \"\"\"\n",
    "    Scale the 'log10_balanced' values in a DataFrame by group.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pandas.DataFrame): The input DataFrame.\n",
    "    - group_column (str): The column name used for grouping.\n",
    "\n",
    "    Returns:\n",
    "    - scaled_matrix (pandas.DataFrame): The DataFrame with scaled 'log10_balanced' values.\n",
    "\n",
    "    \"\"\"\n",
    "    # Obtain the unique values of the group column\n",
    "    group_values = df[group_column].unique()\n",
    "\n",
    "    # Create a list to store the scaled matrices\n",
    "    scaled_matrices = []\n",
    "\n",
    "    # Loop through each group value\n",
    "    for value in group_values:\n",
    "        # Filter the DataFrame by group value\n",
    "        tmp_data = df[df[group_column] == value].copy()\n",
    "\n",
    "        # Scale the log10_balanced values\n",
    "        tmp_data['scaled'] = scaler.fit_transform(tmp_data['log10_balanced'].values.reshape(-1, 1))\n",
    "\n",
    "        # Append the scaled matrix to the list\n",
    "        scaled_matrices.append(tmp_data)\n",
    "\n",
    "    # Concatenate the scaled matrices\n",
    "    scaled_matrix = pd.concat(scaled_matrices)\n",
    "\n",
    "    return scaled_matrix\n",
    "\n",
    "def FilterIdNCells(df, n_cells=5, column='count', plot_histogram=True):\n",
    "    \"\"\"\n",
    "    Filters the DataFrame based on the number of elements per ID and returns the filtered DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame\n",
    "        The input DataFrame containing the data.\n",
    "    - n_cells: int, optional (default=5)\n",
    "        The number of cells required for an interaction ID to be included in the filtered DataFrame.\n",
    "    - column: str, optional (default='count')\n",
    "        The column in the DataFrame used for counting the number of elements per ID.\n",
    "    - plot_histogram: bool, optional (default=True)\n",
    "        Whether to plot a histogram of interaction IDs with less than n_cells elements.\n",
    "\n",
    "    Returns:\n",
    "    - df: DataFrame\n",
    "        The filtered DataFrame containing only the interaction IDs that appear in n_cells cell types.\n",
    "    \"\"\"\n",
    "    # count the number of elements per id\n",
    "    tmp = df.groupby('id')[column].count()\n",
    "    # ids of interaction appearing in n cell types\n",
    "    ids = tmp[tmp == n_cells].index\n",
    "    # filter the data\n",
    "    df = df[df['id'].isin(ids)]\n",
    "    \n",
    "    if plot_histogram:\n",
    "        # histogram of interaction in less than n cell types\n",
    "        plt.hist(tmp[tmp < n_cells], bins=10)\n",
    "        plt.title(f\"Distribution of Interaction IDs with Less than {n_cells} Elements\\nColumn: {column}\")\n",
    "        plt.xlabel(\"Number of Elements\")\n",
    "        plt.ylabel(\"Frequency\")\n",
    "        plt.show()\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Function to replace NaN values with a value outside the range of the data\n",
    "def ReplaceNaNs(df, column='scaled', replace_with=-10):\n",
    "    \"\"\"\n",
    "    Replaces NaN values in a specified column of a DataFrame with a specified value.\n",
    "    \n",
    "    Args:\n",
    "        df (pandas.DataFrame): The DataFrame containing the column with NaN values.\n",
    "        column (str): The name of the column to replace NaN values in. Default is 'scaled'.\n",
    "        replace_with (float): The value to replace NaN values with. Default is -10.\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: The DataFrame with NaN values replaced in the specified column.\n",
    "    \"\"\"\n",
    "        \n",
    "    #if the value is true, throw a warning and change the nan_value to something more appropriate\n",
    "    if df[column].min() < replace_with:\n",
    "        # Change the value based on the minimum value of the scaled data\n",
    "        replace_with = round(df[column].min() - 1)  \n",
    "        warnings.warn(f\"The replace_with is not outside the range of the scaled data. Changing the value to: {nan_value}\")\n",
    "        \n",
    "    # replace the NaN values with the nan_value\n",
    "    df[column] = df[column].fillna(replace_with)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def PreprocessPixels(metadata, chromosomes, resolution, test=False):\n",
    "    \"\"\"\n",
    "    Preprocesses the pixels of a matrix for a given cell type. Perform log10 transformation and scaling by chromosome.\n",
    "\n",
    "    Args:\n",
    "        metadata (DataFrame): The metadata containing information about the cell types and mcool file paths.\n",
    "        chromosomes (list): The list of chromosomes to include in the analysis.\n",
    "        resolution (int): The resolution of the mcool.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: The preprocessed and scaled matrix.\n",
    "\n",
    "    \"\"\"\n",
    "    # List to store scaled matrices\n",
    "    scaled_matrices = []\n",
    "\n",
    "    if test:\n",
    "        cell_index = [0]\n",
    "    else:\n",
    "        cell_index = list(range(len(metadata)))\n",
    "\n",
    "    # Loop through each cell type, e.g. each row in the metadata\n",
    "    for i in cell_index:\n",
    "        # Get the current cell type\n",
    "        cell_type = metadata['Biosource'][i]\n",
    "        # Get the current mcool file path\n",
    "        mcool_file = metadata['local_path'][i]\n",
    "           \n",
    "        # Load the mcool file\n",
    "        cool_data = cooler.Cooler(mcool_file + \"::resolutions/\" + str(resolution))\n",
    "            \n",
    "        # Load the balanced matrix\n",
    "        cool_matrix = cool_data.matrix(balance=True, as_pixels=True, join=True)[:]\n",
    "        # Delete the cooler data to save memory\n",
    "        del cool_data\n",
    "        gc.collect()\n",
    "\n",
    "        \n",
    "        # Select pixels of chromosomes of interest\n",
    "        cool_matrix = cool_matrix[cool_matrix['chrom1'].isin([chromosomes])].reset_index(drop=True)\n",
    "        cool_matrix = cool_matrix[cool_matrix['chrom2'].isin([chromosomes])].reset_index(drop=True) \n",
    "            \n",
    "        # remove inter-chromosomal interactions\n",
    "        cool_matrix = cool_matrix[cool_matrix['chrom1'] == cool_matrix['chrom2']].reset_index(drop=True) \n",
    "\n",
    "        # Add a column with the cell type\n",
    "        cool_matrix['cell_type'] = cell_type\n",
    "\n",
    "        # Calculate the log10 of balanced counts\n",
    "        cool_matrix['log10_balanced'] = cool_matrix.groupby('chrom1')['balanced'].transform(lambda x: np.log10(x))\n",
    "            \n",
    "        # Scale the log10 transformed values by chromosome using scale_by_group function\n",
    "        scaled_matrix = ScaleByGroup(cool_matrix, 'chrom1')\n",
    "        # Delete the cool_matrix to save memory\n",
    "        del cool_matrix\n",
    "        gc.collect()\n",
    "\n",
    "        # Append the scaled matrix to the list\n",
    "        scaled_matrices.append(scaled_matrix)\n",
    "        # Delete the scaled_matrix to save memory\n",
    "        del scaled_matrix\n",
    "        gc.collect()\n",
    "\n",
    "\n",
    "    # Concatenate the scaled matrices\n",
    "    scaled_matrices = pd.concat(scaled_matrices)\n",
    "        \n",
    "    return scaled_matrices\n",
    "\n",
    "def GetAverageMatrix(scaled_data, metadata,  plot_histogram=True, replaceNaN_with=-10, filter_nCells=6):\n",
    "\n",
    "    # Let's add an interaction_id, then group by interaction (or loop by interaction) to average the scaled data.\n",
    "    # This can be a bit time consuming for high resolution bins.\n",
    "        \n",
    "    # add interaction id\n",
    "    scaled_data['id'] = scaled_data['chrom1'].astype(str) + '_' + scaled_data['start1'].astype(str) + '_' + scaled_data['chrom2'].astype(str) + '_' + scaled_data['start2'].astype(str)\n",
    "\n",
    "    #Filter interactions that doesn't appear in all cell types using the local function\n",
    "    scaled_data = FilterIdNCells(df = scaled_data, n_cells=filter_nCells, column='count', plot_histogram=plot_histogram)\n",
    "\n",
    "    # Replace NaN values with a value outside the range of the data\n",
    "    scaled_data = ReplaceNaNs(scaled_data, column='scaled', replace_with=replaceNaN_with)\n",
    "    # short test to make sure everything is working\n",
    "    test = scaled_data['scaled'].isna().sum()\n",
    "    if test > 0:\n",
    "        print(f\"There are {test} NaN values in the scaled column after replacing them with an arbitrary value.\")\n",
    "        # stop the loop if there are still NaN values\n",
    "        # break\n",
    "\n",
    "    # Group by interaction id and average the scaled values\n",
    "    mean_values = scaled_data.groupby('id')['scaled'].mean()\n",
    "    # Convert the mean_values series to a dataframe and reset the index\n",
    "    mean_values = mean_values.reset_index()\n",
    "    # Rename the columns\n",
    "    mean_values.columns = ['id', 'mean']\n",
    "\n",
    "    # Create a new dataframe to store the mean values\n",
    "    # we can filter the data for only one cell type and remove the unnecessary columns. Let's reuse the scaled_data dataframe\n",
    "    data_mean = scaled_data[scaled_data['cell_type'] == metadata['Biosource'][0]]\n",
    "    \n",
    "    # select only the columns we need\n",
    "    data_mean = data_mean[['id', 'chrom1', 'start1', 'end1', 'chrom2', 'start2', 'end2']]\n",
    "\n",
    "    # Merge the mean_values with the data_mean dataframe\n",
    "    data_mean = data_mean.merge(mean_values, on='id', how='left')\n",
    "\n",
    "    # Select the columns we need\n",
    "    data_mean = data_mean[['chrom1', 'start1', 'end1', 'chrom2', 'start2', 'end2', 'mean']]\n",
    "\n",
    "    return data_mean\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the metadata. We need to skip the first row because it is a comment. \n",
    "# We use the first row as header\n",
    "metadata = pd.read_csv(data_dir + \"/metadata.tsv\", sep=\"\\t\", header=0, skiprows=1)[:]\n",
    "# Select the columns of interest\n",
    "metadata = metadata[['File Accession', 'Biosource']]\n",
    "# Remove the rows with NaN values\n",
    "metadata = metadata.dropna()\n",
    "# Add the file path to the metadata\n",
    "metadata['local_path'] = data_dir + metadata['File Accession'] + \".mcool\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1000,\n",
       " 2000,\n",
       " 5000,\n",
       " 10000,\n",
       " 25000,\n",
       " 50000,\n",
       " 100000,\n",
       " 250000,\n",
       " 500000,\n",
       " 1000000,\n",
       " 2500000,\n",
       " 5000000,\n",
       " 10000000]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SameResolutionMcools(metadata['local_path'])\n",
    "GetResolutionsMcool(metadata['local_path'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to loop over the resolutions, scale them and average over cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10000000]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the chromosomes to analyse\n",
    "chromosomes = ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17','chr18','chr19','chr20','chr21','chr22','chrX']\n",
    "\n",
    "# Define the resolutions to use\n",
    "resolutions = GetResolutionsMcool(metadata['local_path'][0])\n",
    "\n",
    "#test resolution\n",
    "#arrange resolution from high to low\n",
    "resolutions.sort(reverse=True)\n",
    "resolutions = [10000000]\n",
    "# resolutions = [ 100000, 50000, 25000, 10000,  5000, 2000, 1000]\n",
    "# resolutions = ['1000', '10000', '100000', '1000000', '2000', '25000', '250000', '2500000', '5000', '50000', '500000', '5000000']\n",
    "resolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to further optimize the process by scaling and averaing by chromosome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available resolutions: [10000000]\n",
      "Processing resolution: 10000000\n",
      "Processing chromosome: chr1\n",
      "Preprocessing pixels...\n",
      "Getting average matrix by chromosome...\n",
      "Processing chromosome: chr2\n",
      "Preprocessing pixels...\n",
      "Getting average matrix by chromosome...\n",
      "Processing chromosome: chr3\n",
      "Preprocessing pixels...\n",
      "Getting average matrix by chromosome...\n",
      "Processing chromosome: chr4\n",
      "Preprocessing pixels...\n",
      "Getting average matrix by chromosome...\n",
      "Processing chromosome: chr5\n",
      "Preprocessing pixels...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_813/1352721198.py:154: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[column] = df[column].fillna(replace_with)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting average matrix by chromosome...\n",
      "Processing chromosome: chr6\n",
      "Preprocessing pixels...\n",
      "Getting average matrix by chromosome...\n",
      "Processing chromosome: chr7\n",
      "Preprocessing pixels...\n",
      "Getting average matrix by chromosome...\n",
      "Processing chromosome: chr8\n",
      "Preprocessing pixels...\n",
      "Getting average matrix by chromosome...\n",
      "Processing chromosome: chr9\n",
      "Preprocessing pixels...\n",
      "Getting average matrix by chromosome...\n",
      "Processing chromosome: chr10\n",
      "Preprocessing pixels...\n",
      "Getting average matrix by chromosome...\n",
      "Processing chromosome: chr11\n",
      "Preprocessing pixels...\n",
      "Getting average matrix by chromosome...\n",
      "Processing chromosome: chr12\n",
      "Preprocessing pixels...\n",
      "Getting average matrix by chromosome...\n",
      "Processing chromosome: chr13\n",
      "Preprocessing pixels...\n",
      "Getting average matrix by chromosome...\n",
      "Processing chromosome: chr14\n",
      "Preprocessing pixels...\n",
      "Getting average matrix by chromosome...\n",
      "Processing chromosome: chr15\n",
      "Preprocessing pixels...\n",
      "Getting average matrix by chromosome...\n",
      "Processing chromosome: chr16\n",
      "Preprocessing pixels...\n",
      "Getting average matrix by chromosome...\n",
      "Processing chromosome: chr17\n",
      "Preprocessing pixels...\n",
      "Getting average matrix by chromosome...\n",
      "Processing chromosome: chr18\n",
      "Preprocessing pixels...\n",
      "Getting average matrix by chromosome...\n",
      "Processing chromosome: chr19\n",
      "Preprocessing pixels...\n",
      "Getting average matrix by chromosome...\n",
      "Processing chromosome: chr20\n",
      "Preprocessing pixels...\n",
      "Getting average matrix by chromosome...\n",
      "Processing chromosome: chr21\n",
      "Preprocessing pixels...\n",
      "Getting average matrix by chromosome...\n",
      "Processing chromosome: chr22\n",
      "Preprocessing pixels...\n",
      "Getting average matrix by chromosome...\n",
      "Processing chromosome: chrX\n",
      "Preprocessing pixels...\n",
      "Getting average matrix by chromosome...\n"
     ]
    }
   ],
   "source": [
    "print(f\"Available resolutions: {resolutions}\")\n",
    "\n",
    "# For each resolution, get the average matrix and save it to a bed file\n",
    "for resolution in resolutions: \n",
    "\n",
    "    # If average matrix already exists, skip the resolution\n",
    "    if os.path.exists(out_dir + f\"average_matrix_{resolution}.bed\"):\n",
    "        print(f\"Average matrix for resolution {resolution} already exists. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    print(f\"Processing resolution: {resolution}\")\n",
    "\n",
    "    # Create a list to store the average matrices by chromosome \n",
    "    average_matrices = []\n",
    "\n",
    "    # To optimize the process, let's try to do it within a loop by chromosome\n",
    "    for chromosome in chromosomes:\n",
    "\n",
    "        print(f\"Processing chromosome: {chromosome}\")\n",
    "        \n",
    "        print(\"Preprocessing pixels...\")\n",
    "        # Preprocess the pixels\n",
    "        scaled_data = PreprocessPixels(metadata, chromosome, resolution, test=False)\n",
    "        gc.collect()\n",
    "\n",
    "        print(\"Getting average matrix by chromosome...\")\n",
    "        \n",
    "        # # Filter the data by chromosome\n",
    "        # scaled_data = scaled_data[scaled_data['chrom1'] == chromosome]\n",
    "        \n",
    "        # Get the average matrix\n",
    "        average_matrix = GetAverageMatrix(scaled_data, metadata, replaceNaN_with=-10, plot_histogram=False, filter_nCells=len(metadata))\n",
    "        \n",
    "        # Remove the scaled data to save memory\n",
    "        # del scaled_data\n",
    "        gc.collect()\n",
    "\n",
    "        # Append the average matrix to the list\n",
    "        average_matrices.append(average_matrix)\n",
    "        \n",
    "        # Save the average matrix to a bed file\n",
    "        average_matrix.to_csv(out_dir + f\"/tmp/average_matrix_{resolution}_{chromosome}.bed\", sep='\\t', index=False, header=False)\n",
    "\n",
    "    # Concatenate the average matrices\n",
    "    average_matrix = pd.concat(average_matrices)\n",
    "    \n",
    "    # Save the average matrix to a bed file\n",
    "    average_matrix.to_csv(out_dir + f\"average_matrix_{resolution}.bed\", sep='\\t', index=False, header=False)\n",
    "    \n",
    "    # Remove the average matrices to save memory\n",
    "    del average_matrices\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the old average matrix to compare\n",
    "old_average_matrix = pd.read_csv(out_dir + \"average_matrix_10000000.bed\", sep=\"\\t\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "2601\n"
     ]
    }
   ],
   "source": [
    "print(sum(old_average_matrix[6].values == average_matrix['mean'].values))\n",
    "print(len(old_average_matrix))\n",
    "#number of rows in the old average matrix\n",
    "# len(old_average_matrix)\n",
    "# len(average_matrix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I obtained some unmatched values, let's explore them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the index of the rows that are different\n",
    "index = np.where(old_average_matrix[6].values != average_matrix['mean'].values)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They look normal to me.. maybe just small decimals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.881784197001252e-16"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the abs difference between the old and new average matrices\n",
    "abs_diff = abs(old_average_matrix[6].values - average_matrix['mean'].values)\n",
    "abs_diff.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, the difference is reeeeealy small. Let's try to add this code to a .py script!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I coded and tested in a super computer. I want to compare their results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 19] No such device: '/mnt/d/Projects/HK_Interactions/Data/HiC/4DN_portal/average/average_matrix_10000000.bed'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#load the old average matrix to compare\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m old_average_matrix \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maverage_matrix_10000000.bed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\t\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/HK_PPI/lib/python3.10/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/HK_PPI/lib/python3.10/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/HK_PPI/lib/python3.10/site-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    947\u001b[0m )\n\u001b[1;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/HK_PPI/lib/python3.10/site-packages/pandas/io/parsers/readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    602\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    604\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 605\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/miniconda3/envs/HK_PPI/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/HK_PPI/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1734\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1738\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1739\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1740\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1741\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1742\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/miniconda3/envs/HK_PPI/lib/python3.10/site-packages/pandas/io/common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    852\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    853\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    855\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 856\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    864\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    865\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 19] No such device: '/mnt/d/Projects/HK_Interactions/Data/HiC/4DN_portal/average/average_matrix_10000000.bed'"
     ]
    }
   ],
   "source": [
    "#load the old average matrix to compare\n",
    "old_average_matrix = pd.read_csv(out_dir + \"average_matrix_10000000.bed\", sep=\"\\t\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HK_PPI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
